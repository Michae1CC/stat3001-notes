\subsection*{Methods of Finding Estimates Introduction}

\begin{defe}[Statistic] \label{defe: statistic}
    Let $X_1, \ldots , X_n$ be a random sample of size $n$ from a population and let $T(x_1, \ldots , x_n)$ be a real-valued or vector-valued function whose domain includes the sample space of $(X_1, \ldots , X_n)$. The the random variable or random vector $Y = T(X_1, \ldots , X_n)$ is called a {\bf statistic}. The probability distribution of a statistic $Y$ is called the {\bf sampling distribution} of $Y$ \cite{CasellaGeorge2001SI}*{page 211}.
\end{defe}

\begin{defe}[Sample Mean] \label{defe: sample_mean}
    The {\bf sample mean} is the arthicmetic average of the values in a random sample. It is usually denoted by
    \begin{equation}
        \overline{X} = \frac{1}{n} \sum_{i=1}^{n} X_i
    \end{equation}
    \cite{CasellaGeorge2001SI}*{page 212}.
\end{defe}

\begin{defe}[Sample Variance and Standard Deviation] \label{defe: var_std_dev}
    The {\bf sample variance} is the statistic defined by
    \begin{equation}
        S^2 = \frac{1}{n-1} \sum_{i=1}^{n} (X_i - \overline{X})^2.
    \end{equation}
    The {\bf sample standard deviation} is the statistic defined by $S = \sqrt{S^2}$ \cite{CasellaGeorge2001SI}*{page 212}.
\end{defe}

\begin{defe}[Sufficient Statistic] \label{defe: sufficient_statistic}
    A statistic $T(\bm{X})$ is a {\bf sufficient statistic} for $\theta$ if the conditional distribution of the sample $\bm{X}$ given the value of $T(\bm{X})$ does not depend on $\theta$ \cite{CasellaGeorge2001SI}*{page 272}.
\end{defe}

\begin{thm} \label{thm: sufficient_statistic_ratio}
    If $p(\bm{x} \mid \theta)$ is the joint pdf or pmf of $\bm{X}$ and $q(\theta \mid \theta)$ is the pdf or pmf of $T(\bm{X})$, then $T(\bm{X})$ is a sufficient statistic for $\theta$ if, for every $\bm{x}$ in the sample space, the ratio $p(\bm{x} \mid \theta) / q(T(\bm{x}) \mid \theta)$ is a constant function of $\theta$ \cite{CasellaGeorge2001SI}*{page 274}.
\end{thm}

\begin{thm}[Factorization Theorem] \label{thm: factorization_theorem}
    Let $f(\bm{x} \mid \theta)$ denote the joint pdf or pmf of a sample $\bm{X}$. A statistic $T(\bm{X})$ is a sufficient statistic for $\theta$, if and only if there exist function $g(t\mid \theta)$ and $h(\bm{x})$ such that, for all sample points $\bm{x}$ and all parameter points $\theta$,
    \begin{equation*}
        f(\bm{x} \mid \theta) = g(T(\bm{x}) \mid \theta) h(\bm{x})
    \end{equation*}
    \cite{CasellaGeorge2001SI}*{page 276}.
\end{thm}

\begin{exam}[Uniform Sufficient Statistic] \label{exam: uni_ss_p1}
    Example taken from \cite{CasellaGeorge2001SI}*{page 277} and can also be found on tutorial sheet 3. Let $X_1 , \ldots , X_n$ be iid observations from the discrete uniform distribution on $1 , \ldots , \theta$. That is, the unknown parameter, $\theta$, is a positive integer and the pmf of $X_i$ is
    \begin{equation*}
        f(x \mid \theta) =
        \left\{
        \begin{matrix}
            \frac{1}{\theta}, & x = 1,2,\ldots \theta \\
            0,                & \text{otherwise}
        \end{matrix}
        \right. .
    \end{equation*}
    The restriction $x_i \in \left\{ 1, \ldots ,\theta \right\}$ for $i=1,\ldots ,n$ can be re-expressed as $x_i \in \left\{ 1,2 , \ldots \right\}$ for $i = 1,\ldots n$ (note that there is no $\theta$ in this restriction) and $\max_i x_i \leq \theta$. If we define $T(\bm{x}) = \max_i x_i = x_{(n)}$,
    \begin{equation*}
        h(x) =
        \left\{
        \begin{matrix}
            1, & x_i \in \left\{ 1, \ldots ,\theta \right\} \; \text{for} \; i=1,\ldots ,n \\
            0, & \text{otherwise}
        \end{matrix}
        \right.
    \end{equation*}
    and
    \begin{equation*}
        g(t \mid \theta) =
        \left\{
        \begin{matrix}
            \theta^{-n} & t \leq \theta    \\
            0,          & \text{otherwise}
        \end{matrix}
        \right. .
    \end{equation*}
    It is easily verified that $f(\bm{x} \mid \theta) = g(T(\bm{x}) \mid \theta)$ for all $\bm{x}$ and $\theta$. Thus, according to \Cref{thm: factorization_theorem}, the largest order statistic, $T(\bm{X}) = X_{(n)}$, is a sufficient statistic in this problem. This type of analysis can sometimes be carried out more clearly and concisely using indicator function. Let $\NN$ be the set of natural numbers (discluding $0$) and $\NN_{\theta}$ be the natural numbers up to and including $\theta$. Then the joint pmf of $X_1 , \ldots , X_n$ is
    \begin{equation*}
        f(\bm{x} \mid \theta) = \prod_{i=1}^{n} \theta^{-1} \Id_{N_\theta} (x_i) = \theta^{-n} \prod_{i=1}^{n} \Id_{N_\theta} (x_i).
    \end{equation*}
    Defining $T(\bm{x}) = x_{(n)}$, we see that
    \begin{equation*}
        \prod_{i=1}^{n} \Id_{N_\theta} (x_i) = \left( \prod_{i=1}^{n} \Id_{N} (x_i) \right) \Id_{N_\theta} (T(x))
    \end{equation*}
    thus providing the factorization
    \begin{equation*}
        f(\bm{x} \mid \theta) = \theta^{-n} \Id_{N_\theta} (T(x)) \left( \prod_{i=1}^{n} \Id_{N} (x_i) \right).
    \end{equation*}
    The first factor depends on $x_1 , \ldots , x_n$ only through the value of $T(\bm{x}) = x_{(n)}$, and the second factor does not depend on $\theta$. Again, according to \Cref{thm: factorization_theorem}, $T(\bm{X}) = X_{(n)}$, is a sufficient statistic in this problem.
\end{exam}

\begin{defe}[Likelihood, Log-Likelihood and Score Function] \label{defe: likelihood_function}
    Let $f(\bm{x} \mid \theta)$ denote the joint pdf or pmf of the sample $\bm{X} = (X_1 , \ldots , X_2)$. Then, given that $\bm{X} = \bm{x}$ is observed, the function of $\theta$ defined by
    \begin{equation*}
        L(\theta \mid \bm{x}) = f(\bm{x} \mid \theta)
    \end{equation*}
    is called the {\bf likelihood function} \cite{CasellaGeorge2001SI}*{page 290}. For a given outcome $\bm{x}$ of $\bm{X}$, the {\bf log-likelihood function}, denoted $l$, is the natural logarithm of the likelihood function
    \begin{equation*}
        l(\theta \mid \bm{x}) = \ln L(\theta \mid \bm{x}) = \ln f(\bm{x} \mid \theta).
    \end{equation*}
    It's gradient with respect to $\theta$, denoted $S$, is called the {\bf score function}
    \begin{equation*}
        S (\theta \mid \bm{x}) = \nabla_{\theta} \, l (\theta \mid \bm{x}) \frac{\nabla_{\theta} f(\bm{x} \mid \theta)}{f(\bm{x} \mid \theta)}
    \end{equation*}
    \cite{KroeseDirkP2013SMaC}*{page 165}.
\end{defe}

\begin{thm} \label{thm: e_score}
    Under regularity conditions
    \begin{equation*}
        \EE \left[ S (\theta \mid \bm{x}) \right] = 0
    \end{equation*}
    [Background Notes, page 10].
\end{thm}

\begin{proof}
    Since $L(\theta)$ is a density when viewed as a function of the observed data $x_1 , \ldots , x_n$ we have the following identity in $\theta$,
    \begin{equation*}
        \int \cdots \int L(\theta) \; d x_1 \; \ldots \; d x_n = 1.
    \end{equation*}
    On differentiating both sides of the above with respect to $\theta$ gives
    \begin{equation*}
        \int \cdots \int \left[ \frac{\partial L(\theta) }{\partial \theta} \right] \; d x_1 \; \ldots \; d x_n = 0.
    \end{equation*}
    Apply the chain rule to $\frac{\partial \ln L(\theta) }{\partial \theta}$ we find
    \begin{equation*}
        \frac{\partial \ln L(\theta) }{\partial \theta} = \frac{\partial \ln L(\theta) }{\partial L (\theta)} \cdot \frac{\partial L(\theta) }{\partial \theta} = \frac{1}{L(\theta)} \frac{\partial L(\theta) }{\partial \theta}
    \end{equation*}
    meaning
    \begin{equation*}
        \frac{\partial \ln L(\theta) }{\partial \theta} L(\theta) = \frac{\partial L(\theta) }{\partial \theta}
    \end{equation*}
    so that
    \begin{align*}
        \int \cdots \int \left[ \frac{\partial L(\theta) }{\partial \theta} \right] \; d x_1 \; \ldots \; d x_n               & = 0 \\
        \int \cdots \int \left[ \frac{\partial \ln L(\theta) }{\partial \theta} \right] L(\theta) \; d x_1 \; \ldots \; d x_n & = 0 \\
        \EE \left[ S (\theta) \right]                                                                                         & = 0
    \end{align*}
    as wanted.
\end{proof}

\begin{defe}[Expotential Family] \label{defe: exp_fam}
    In the case of $p-$dimensional observation $\bm{x}_1 , \bm{x}_2 , \ldots , \bm{x}_n \in \CC^{p}$, a $d-$dimensional parameter vector $\bm{\theta} \in \CC^{d}$, and a $q-$dimensional sufficient statistic $T(\bm{x}_1 , \ldots , \bm{x}_n) \in \CC^{q}$, the likelihood function $L (\bm{\theta})$ for the $d-$parameter vector $\bm{\theta}$ has the following form if it belongs to the $d-$parameter {\bf exponential family}
    \begin{equation*}
        L (\bm{\theta}) = b(\bm{x}_1 , \ldots , \bm{x}_n) \exp \left\{ c(\bm{\theta})^{\intercal} T(\bm{x}_1 , \ldots , \bm{x}_n) \right\} / a(\bm{\theta})
    \end{equation*}
    where $c(\bm{\theta}) \in \CC^{q}$ and $b(\bm{x}_1 , \ldots , \bm{x}_n)$ and $a(\bm{\theta})$ are scalar functions \cite{CasellaGeorge2001SI}*{page 279}.
\end{defe}

\begin{thm} \label{thm: exp_fam_ss}
    Let $\bm{X}_1 , \bm{X}_2 , \ldots , \bm{X}_n$ be iid observations from a pdf or pmf $f(x \mid \bm{\theta})$ that belongs to an exponential family as seen in \Cref{defe: exp_fam}, then
    \begin{equation*}
        T(\bm{X}_1 , \ldots , \bm{X}_n) = \left( \sum_{j=1}^{n} t_1 (\bm{X}_j) , \ldots , \sum_{j=1}^{n} t_k (\bm{X}_j) \right)
    \end{equation*}
    is a sufficient statistic for $\bm{\theta}$ \cite{CasellaGeorge2001SI}*{page 279}.
\end{thm}

\begin{defe}[Minimal Sufficient Statistic] \label{defe: minimal_sufficient_statistic}
    A sufficient statistic $T(\bm{X})$ is called a {\bf minimal sufficient statistic} if, for any other sufficient statistic $T'(\bm{X})$, $T(\bm{x})$ is a function of $T'(\bm{x})$ \cite{CasellaGeorge2001SI}*{page 280}.
\end{defe}

\begin{thm} \label{thm: mss_ratio_test}
    Let $f(\bm{x} \mid \theta)$ be the pd of a sample $\bm{X}$. Suppose there exists a function $T(\bm{x})$ such that, for every two sample points $\bm{x}$ and $\bm{y}$, the ratio $f(\bm{x} \mid \theta) / f(\bm{y} \mid \theta)$ is constant as a function of $\theta$ if and only if $T(\bm{x}) = T(\bm{y})$. Then $T(\bm{X})$ is a minimal sufficient statistic \cite{CasellaGeorge2001SI}*{page 281}.
\end{thm}

\begin{exam}[Normal Minimal Sufficient Statistic] \label{exam: norm_min_ss}
    Example taken from \cite{CasellaGeorge2001SI}*{page 281}. Let $X_1 , \ldots , X_n \iid \Nor (\mu , \sigma^2)$, where both $\mu$ and $\sigma^2$ unknown. Let $\bm{x}$ and $\bm{y}$ denote two sample points, and let $(\overline{x}, s_{\bm{x}}^2)$ and $(\overline{y}, s_{\bm{y}}^2)$ be the sample means and variances corresponding to the $\bm{x}$ and $\bm{y}$ samples, respectively. Then, the ratio of the densities becomes
    \begin{align*}
        \frac{f\left(\mathbf{x} \mid \mu, \sigma^{2}\right)}{f\left(\mathbf{y} \mid \mu, \sigma^{2}\right)} \
         & =\frac{\left(2 \pi \sigma^{2}\right)^{-n / 2} \exp \left(-\left[n(\bar{x}-\mu)^{2}+(n-1) s_{\mathbf{x}}^{2}\right] /\left(2 \sigma^{2}\right)\right)}{\left(2 \pi \sigma^{2}\right)^{-n / 2} \exp \left(-\left[n(\bar{y}-\mu)^{2}+(n-1) s_{\mathbf{y}}^{2}\right] /\left(2 \sigma^{2}\right)\right)} \\
         & =\exp \left(\left[-n\left(\bar{x}^{2}-\bar{y}^{2}\right)+2 n \mu(\bar{x}-\bar{y})-(n-1)\left(s_{\mathbf{x}}^{2}-s_{\mathbf{y}}^{2}\right)\right] /\left(2 \sigma^{2}\right)\right) .
    \end{align*}
    This ratio will be constant as a function of $\mu$ and $\sigma^2$ if and only if $\overline{x}=\overline{y}$ and $s_{\bm{x}}^2 = s_{\bm{y}}^2$. Thus by \Cref{thm: mss_ratio_test}, $(\overline{X}, S^2)$ is a minimal sufficient statistic for $(\mu , \sigma^2 )$.
\end{exam}

\begin{defe}[Ancillary Statistic] \label{defe: ancillary_statistic}
    A statistic $S(\bm{X})$ whose distribution does not depend on the parameter $\theta$ is called an ancillary statistic \cite{CasellaGeorge2001SI}*{page 282}.
\end{defe}

\begin{defe}[Complete Distributions and Statistics] \label{defe: complete_statistic}
    Let $f(t \mid \theta)$ be a family of pdfs or pmfs for a statistic $T(\bm{X})$. The family of probability distributions is called {\bf complete} if $\EE_{\theta} g(T) = 0$, for some function $g$, for all $\theta$ implies $\PP (g(T) = 0) = 1$ for all $\theta$. Equivalently, $T(\bm{X})$ is called a {\bf complete statistic} \cite{CasellaGeorge2001SI}*{page 285}.
\end{defe}

\begin{thm} \label{thm: exp_fam_complete}
    Let $\bm{X}_1 , \bm{X}_2 , \ldots , \bm{X}_n$ be iid observations from a pdf or pmf $f(x \mid \bm{\theta})$ that belongs to an exponential family as seen in \Cref{defe: exp_fam}, then the statistic
    \begin{equation*}
        T(\bm{X}_1 , \ldots , \bm{X}_n) = \left( \sum_{j=1}^{n} t_1 (\bm{X}_j) , \ldots , \sum_{j=1}^{n} t_k (\bm{X}_j) \right)
    \end{equation*}
    is complete as long as the parameter space is non-meager \cite{CasellaGeorge2001SI}*{page 288}.
\end{thm}

\begin{thm} \label{thm: min_and_complete}
    If a minimal sufficient statistic exists, then any complete statistic is also a minimal sufficient statistic \cite{CasellaGeorge2001SI}*{page 289}.
\end{thm}

\begin{thm} \label{thm: suf_comp_min}
    A complete, sufficient statistic is always minimal [Background Notes, page 25].
\end{thm}

\begin{exam}[Binomial Complete Statistic] \label{exam: bin_comp_stat}
    Example taken from \cite{CasellaGeorge2001SI}*{page 285}. Suppose that $T$ has a $\Bin (n,p)$ distribution, $0<p<1$. Let $g$ be a function such that $\EE_{p} g(T) = 0$. Then
    \begin{align*}
        0 = \EE_{p} g(T) & = \sum_{t=0}^{n} g(t) \binom{n}{t} p^{t} (1-p)^{n-1}                        \\
                         & = (1-p)^n \sum_{t=0}^{n} g(t) \binom{n}{t} \left( \frac{p}{1-p} \right)^{t}
    \end{align*}
    for all $p$, $0<p<1$. The factor $(1-p)^n$ is not 0 for any $p$ in this range. Thus it must be that
    \begin{equation*}
        0 = \sum_{t=0}^{n} g(t) \binom{n}{t} \left( \frac{p}{1-p} \right)^{t} = \sum_{t=0}^{n} g(t) \binom{n}{t} r^t
    \end{equation*}
    for all, $0 < r < \infty$. But the last expression is a polynomial of degree $n$ in $r$, where the coefficient of $r^t$ is $g(t) \binom{n}{t}$. For the polynomial to be $0$ for all $r$, each coefficient must be $0$. Since none of the $\binom{n}{t}$ terms is $0$, this implies that $g(t) = 0$ for $t=0,1,\ldots n$. Since $T$ takes on the values $0,1,\ldots n$ with probability $1$, this means that $\PP_p (g(T) = 0) = 1$ for all $p$, the desired conclusion. Hence, $T$ is a complete statistic.
\end{exam}

\begin{exam}[Sum of iid Bernoulli RVs] \label{exam: sum_ber_rv}
    Example taken from [Tutorial Sheet 2, Q6]. Let $X_1 , \ldots , X_n \iid \Ber (\theta)$. The likelihood function for $\theta$ is given by
    \begin{align*}
        L (\theta) \
         & = \prod_{j=1}^{n} \binom{n}{x_j} \theta^{x_j} (1-\theta)^{1-x_j}                             \\
         & = \left[ \prod_{j=1}^{n} \binom{n}{x_j} \right] \theta^{t} (1-\theta)^{n - t}                \\
         & = \left[ \prod_{j=1}^{n} \binom{n}{x_j} \right] \exp \left[ c(\theta) t \right] (1-\theta)^n \\
         & = b (\bm{x}) \exp \left[ c(\theta) t \right] / a (\theta)
    \end{align*}
    where
    \begin{align*}
        t (\bm{X}) & = \sum_{i=1}^{n} X_i              \\
        c(\theta)  & = \ln \frac{\theta}{1-\theta}     \\
        a(\theta)  & = (1-\theta)^{-n}                 \\
        b(\bm{x})  & = \prod_{j=1}^{n} \binom{n}{x_j}.
    \end{align*}
    Clearly, the likelihood belongs to the regular exponential family with canonical parameter $c(\theta)$ and complete sufficient statistic $T = t (\bm{X})$. Also, the score statistic (\Cref{defe: likelihood_function}) is given by
    \begin{equation*}
        S(\theta) = \frac{\partial}{\partial \theta} \ln L(\theta) = \frac{n}{\theta (1-\theta)} \left( \frac{t}{n} - \theta \right)
    \end{equation*}
    showing that the estimator $T$ attains the Cramer-Rao lower bound is estimating $\theta$. Hence, it attains the MVB (\Cref{cor: cri_attainment}) and is therefore also a UMVU estimator of $\theta$. On the other hand, the estimator
    \begin{equation*}
        V = \left( X_n , T_{n-1} \right)^{\intercal}
    \end{equation*}
    where $T_{n-1} = \sum_{j=1}^{n-1} X_j$, while sufficient (with canonical parameter $c(\theta)= (\ln \frac{\theta}{1-\theta} , \ln \frac{\theta}{1-\theta})^{\intercal}$), is not complete. To demonstrate that $V$ is not complete, we have that
    \begin{equation*}
        \EE \left[ X_n - \frac{1}{n-1} T_{n-1} \right] = 0
    \end{equation*}
    however, consider
    \begin{align*}
        \PP \left[ X_n - \frac{1}{n-1} T_{n-1} = 0 \right].
    \end{align*}
    Since, $X_n \sim \Ber (\theta)$, $T_{n-1} \sim \Bin (n-1, \theta)$ and $X_i$ are iid
    \begin{align*}
        \PP \left[ X_n - \frac{1}{n-1} T_{n-1} = 0 \right] \
         & = \PP \left[ T_{n-1} = 0 \mid X_n = 0 \right] \cdot \PP \left[ X_n = 0 \right] + \PP \left[ T_{n-1} = n-1 \mid X_n = 0 \right] \cdot \PP \left[ X_n = 1 \right] \\
         & = (1 - \theta)^{n} + \theta^{n} \neq 1
    \end{align*}
    for $0 < \theta < 1$. So by \Cref{defe: complete_statistic}, $V$ is not complete. Furthermore, as $T$ is a complete, sufficient statistic, it is a minimal sufficient statistic (\Cref{thm: suf_comp_min}) for $\theta$. It is a function of every other sufficient statistic (\Cref{defe: minimal_sufficient_statistic}) and here we can see it is a function of $V$ with
    \begin{equation*}
        T = (V)_1 + (V)_2 = X_n + T_{n-1}.
    \end{equation*}
    This also shows that $V$ is not a (sufficient) minimal statistic (again by \Cref{defe: minimal_sufficient_statistic}). Now lets consider the variance between two estimators of $\theta$, $T = \frac{1}{n} \sum_{i=1}^{n} X_i$ and $W(V) = \EE \left[ X_1 \mid V \right]$. We saw that $T$ is UMVU and its variance attains MVB. Its variance can be computed as
    \begin{equation*}
        \Var (T) = \frac{1}{n^2} (n \theta (1-\theta)) = \frac{1}{n} \theta (1 - \theta).
    \end{equation*}
    Now let us try and find an explicit espression for $W(V(\bm{x}))$. We have
    \begin{align*}
        W(V(\bm{x})) \
         & = \EE \left[ X_1 \mid X_n = x_n , \sum_{i=1}^{n-1} X_i = t_{n-1} \right]                                                                                                                                  \\
         & = \sum_{x_1 = 0}^{1} x_1 \cdot \PP \left[ X_1 = x_1 \mid X_n = x_n , \sum_{i=1}^{n-1} X_i = t_{n-1} \right]                                                                                               \\
         & = \PP \left[ X_1 = 1 \mid X_n = x_n , \sum_{i=1}^{n-1} X_i = t_{n-1} \right]                                                                                                                              \\
         & = \frac{\PP \left[ X_1 = 1 , X_n = x_n , \sum_{i=1}^{n-1} X_i = t_{n-1} \right]}{\PP \left[ X_n = x_n , \sum_{i=1}^{n-1} X_i = t_{n-1} \right]}                                                           \\
         & = \frac{\PP \left[ X_1 = 1 , X_n = x_n , \sum_{i=2}^{n-1} X_i = t_{n-1} - 1 \right]}{\PP \left[ X_n = x_n , \sum_{i=1}^{n-1} X_i = t_{n-1} \right]}                                                       \\
         & = \frac{\PP \left[ X_1 = 1 \right] \PP \left[  X_n = x_n \right]  \PP \left[ \sum_{i=2}^{n-1} X_i = t_{n-1} - 1 \right]}{\PP \left[ X_n = x_n \right] \PP \left[ \sum_{i=1}^{n-1} X_i = t_{n-1} \right]}.
    \end{align*}
    Since $X_1 \sim \Ber (\theta)$, $\sum_{i=1}^{n-1} X_i \sim \Bin (n-1,\theta)$ and $\sum_{i=2}^{n-1} X_i \sim \Bin (n-2,\theta)$, we have
    \begin{align*}
        W(V(\bm{x})) \
         & = \frac{\theta \binom{n-2}{t_{n-1}-1} \theta^{t_{n-1}-1} {(1-\theta)}^{(n-2)-(t_{n-1}-1)}}{\binom{n-1}{t_{n-1}} \theta^{t_{n-1}} (1-\theta)^{(n-1)-t_{n-1}}} \\
         & = t_{n-1}/(n-1)
    \end{align*}
    where $t_{n-1} = \sum_{i=1}^{n-1} x_i$. This means $W(V(\bm{X})) = \frac{1}{n-1} \sum_{i=1}^{n-1} X_i$ and
    \begin{equation*}
        \Var \left( W(V) \right) = \frac{(n-1)}{(n-1)^2} \theta (1-\theta) = \frac{1}{(n-1)} \theta (1 - \theta) < \frac{1}{n} \theta (1 - \theta).
    \end{equation*}
\end{exam}

\begin{defe}[Point Estimator] \label{defe: point_estimator}
    A {\bf point estimator} is any function $W(X_1 , \ldots , X_n)$ of a sample; that is, any statistic (see \Cref{defe: statistic}) is a point estimator \cite{CasellaGeorge2001SI}*{page 311}.
\end{defe}

\begin{defe}[Fisher Information Matrix] \label{defe: fim}
    For the model $\bm{X} \sim f(\cdot ; \bm{\theta})$, let $S(\bm{\theta})$ be the score function (see \Cref{defe: likelihood_function}) of $\bm{\theta}$. The covariance matrix of the random vector $S(\bm{\theta})$, denoted by $\calJ (\theta)$, is called the {\bf Fisher Information Matrix} where
    \begin{equation*}
        \calJ (\bm{\theta}) = \EE_{\bm{\theta}} \left[ S(\bm{\theta}) S(\bm{\theta})^{\intercal} \right]
    \end{equation*}
    in the multivariate case and
    \begin{equation*}
        \calJ (\theta) = \EE_{\theta} \left( \frac{d}{d \theta} \ln f(\bm{X} ; \theta) \right)^2
    \end{equation*}
    in the one-dimensional case. Note that under regularity conditions $\EE \left[ S (\theta) \right] = 0$ (see \Cref{thm: e_score}) so that
    \begin{align*}
        \calJ (\theta) \
         & = \EE_{\theta} \left[ \frac{d}{d \theta} \ln f(\bm{X} ; \theta) \right]^2                                                                                         \\
         & = \Var_{\theta} \left( \frac{d}{d \theta} \ln f(\bm{X} ; \theta) \right) + \left( \EE_{\theta} \left[ \frac{d}{d \theta} \ln f(\bm{X} ; \theta) \right] \right)^2 \\
         & = \Var_{\theta} \left( S (\theta) \right) + \left( \EE_{\theta} \left[ S (\theta) \right] \right)^2                                                               \\
         & = \Var_{\theta} \left( S (\theta) \right)
    \end{align*}
    \cite{KroeseDirkP2013SMaC}*{page 168}.
\end{defe}

\begin{defe}[Observed Information] \label{defe: oim}
    For the model $\bm{X} \sim f(\cdot ; \bm{\theta})$, let $S(\bm{\theta})$ be the score function (see \Cref{defe: likelihood_function}) of $\bm{\theta}$. The negative of the Hessian of the random vector $S(\bm{\theta})$, denoted by $I (\theta)$, is called the {\bf Observed Information} where
    \begin{equation*}
        I (\bm{\theta}) = - \nabla \nabla S(\bm{\theta})
    \end{equation*}
    in the multivariate case and
    \begin{equation*}
        I (\bm{\theta}) = - \frac{\partial^2}{\partial \theta^2} \ln f(\bm{X} ; \theta)
    \end{equation*}
    in the one-dimensional case [Background Notes, page 8].
\end{defe}

\begin{thm} \label{thm: fim_and_oim}
    Under regularity conditions, the following equality holds
    \begin{equation*}
        \calJ (\bm{\theta}) = \EE \left[ I (\bm{\theta}) \right]
    \end{equation*}
    \cite{KroeseDirkP2013SMaC}*{page 169}.
\end{thm}

\begin{thm}[Fisher Information Matrix for iid Data] \label{thm: fim_iid}
    Let $\bm{X} = (X_1 , \ldots , X_n) \iid \mathring{f} (x ; \bm{\theta})$, and let $\mathring{\calJ} (\bm{\theta})$ be the information matrix corresponding to $X \sim \mathring{f} (x ; \bm{\theta})$. Then the information matrix for $\bm{X}$ is given by
    \begin{equation*}
        \calJ (\bm{\theta}) = n \mathring{\calJ} (\bm{\theta})
    \end{equation*}
    \cite{KroeseDirkP2013SMaC}*{page 170}.
\end{thm}

\begin{thm} \label{thm: ll_rc}
    If the $L (\theta)$ belongs to the regular exponential family, then the likelihood equation
    \begin{equation*}
        \frac{d}{d \bm{\theta}} \ln L (\bm{\theta}) = \bm{0},
    \end{equation*}
    can be expressed as
    \begin{equation*}
        T (\bm{x}_1 , \ldots , \bm{x}_n) = \EE \left[ T (\bm{X}_1 , \ldots , \bm{X}_n) \right]
    \end{equation*}
    [Lecture Notes 1, page 8].
\end{thm}