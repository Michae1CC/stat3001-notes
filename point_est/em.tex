\subsection*{Expectation Maximization Algorithm}

The expectation-maximization (EM) algorithm is a broadly applicable approach to the
iterative computation of maximum likelihood (ML) estimates, useful in a variety of incomplete data problems, where algorithms such as the Newton-Raphson method may turn out to be more complicated. On each iteration of the EM algorithm, there are two steps called the Expectation step or the $E-$step and the Maximization step or the $M-$step. Because of this, the algorithm is called the EM algorithm.

\subsubsection*{Formulation of the EM Algorithm}

We let $\bm{Y}$ be the random vector corresponding to the observed data $\bm{y}$ having p.d.f. postulated as $g(\bm{y} ; \bm{\Psi})$, where $\bm{\Psi} = \left( \Psi_1 , \Psi_2 , \ldots , \Psi_d \right)^{\intercal}$ is a vector of unknown parameters with parameter space $\Omega$. We let $g_c (\bm{x};\bm{\Psi})$ denote the p.d.f. of the random vector $\bm{X}$ corresponding to the complete data vector $\bm{x}$. Then the complete-data log likelihood function that could be formed for $\bm{\Psi}$ is $\bm{x}$ were fully observable is given by
\begin{equation*}
    \ln L_{c} (\bm{\Psi}) = \ln g_c (\bm{x} ; \bm{\Psi}).
\end{equation*}
Formally, we have two sample space $X$ and $Y$ and a many-to-one mapping $X$ to $Y$. Instead of observing the complete-data vector $\bm{x} \in X$, we observe the incomplete-data vector $\bm{y} = \bm{y} (\bm{x}) \in Y$. It follows that
\begin{equation*}
    g (\bm{y} ; \bm{\Psi}) = \int_{X(\bm{y})} g_c (\bm{x}; \bm{\Psi}) \; d \bm{x}
\end{equation*}
where $X(\bm{y})$ is the subset of $X$ determined by the equation $\bm{y} = \bm{y} (\bm{x})$. The EM algorithm approaches the problem of solving the incomplete-data likelihood equation
\begin{equation*}
    \nabla_{\bm{\Psi}} \ln L (\bm{\Psi}) = 0
\end{equation*}
indirectly by proceeding iteratively in terms of the complete-data log likelihood function, $\ln L_c (\bm{\Psi})$. As it is unobservable, it is replaced by its conditional expectation given $\bm{y}$, using the current fit for $\bm{y}$. More specifically, let $\bm{\Psi}^{(0)}$ be some initial value for $\bm{\Psi}$. Then on the first iteration, the $E-$step requires the calculation of
\begin{equation*}
    Q \left( \bm{\Psi} ; \bm{\Psi}^{(0)} \right) = \EE_{\bm{\Psi}^{(0)}} \left[ \ln L_c \left( \bm{\Psi} \right) \mid \bm{y} \right].
\end{equation*}
The $M-$step requires the maximization of $Q \left( \bm{\Psi} ; \bm{\Psi}^{(0)} \right)$ with respect to $\bm{\Psi}$ over the parameter space $\Omega$. That is, we choose $\bm{\Psi}^{(1)}$ such that
\begin{equation*}
    Q \left( \bm{\Psi}^{(1)} ; \bm{\Psi}^{(0)} \right) \geq Q \left( \bm{\Psi} ; \bm{\Psi}^{(0)} \right)
\end{equation*}
for all $\bm{\Psi} \in \Omega$. The $E-$ and $M-$steps are then carried out again, but this time with $\bm{\Psi}^{(0)}$ replaced by the current fit $\bm{\Psi}^{(1)}$. On the $(k+1)^{th}$ iteration, the $E-$ and $M-$steps are defined as follows:

\begin{defe}[$E-$step] \label{defe: e_step}
    Calculate $Q \left( \bm{\Psi} ; \bm{\Psi}^{(k)} \right)$ as
    \begin{equation*}
        Q \left( \bm{\Psi} ; \bm{\Psi}^{(k)} \right) = \EE_{\bm{\Psi}^{(k)}} \left[ \ln L_c \left( \bm{\Psi} \right) \mid \bm{y} \right].
    \end{equation*}
\end{defe}

\begin{defe}[$M-$step] \label{defe: m_step}
    Choose $\bm{\Psi}^{(k+1)}$ to be any value of $\bm{\Psi} \in \Omega$ that maximises $Q \left( \bm{\Psi} ; \bm{\Psi}^{(k)} \right)$, that is,
    \begin{equation*}
        Q \left( \bm{\Psi}^{(k+1)} ; \bm{\Psi}^{(k)} \right) \geq Q \left( \bm{\Psi} ; \bm{\Psi}^{(k)} \right)
    \end{equation*}
    for all $\bm{\Psi} \in \Omega$.
\end{defe}

The $E-$ and $M-$ steps are alternated repeatedly until the difference
\begin{equation*}
    L (\bm{\Psi}^{(k+1)}) - L (\bm{\Psi}^{(k)})
\end{equation*}
changes by an arbitrarily small amount in the case of convergence of the sequence of likelihood value $L (\bm{\Psi}^{(k)})$. Another way of expressing \Cref{defe: e_step} is to say that $\bm{\Psi}^{(k+1)}$ belongs to
\begin{equation*}
    \calM \left( \bm{\Psi}^{(k)} \right) = \operatornamewithlimits{argmax}_{\bm{\Psi}} Q \left( \bm{\Psi} ; \bm{\Psi}^{(k)} \right),
\end{equation*}
which is the set of points that maximise $Q \left( \bm{\Psi} ; \bm{\Psi}^{(k)} \right)$.

\begin{exam}[A Multinomial Example] \label{exam: em_mutlinomial_eg}
    Example taken from \cite{McLachlanGeoffreyJohn2008TEaa}*{page 10} \cite{KroeseDirkP2013SMaC}*{page 185}. We consider first the multinomial example that DLR used to introduce the EM algorithm and that has been subsequently used many times in the literature to illustrate various modifications and extensions of this algorithm. The data relates to a problem of estimation of linkage in genetics where an observed data vector of frequencies
    \begin{equation*}
        \bm{y} = (y_1 , y_2 , y_3 , y_4)^{\intercal}
    \end{equation*}
    is a postulated to arise from a multinomial distribution with four cells with cell probabilities
    \begin{equation*}
        \frac{1}{2} + \frac{1}{4} \Psi , \frac{1}{4} \left( 1 - \Psi \right) , \frac{1}{4} \left( 1 - \Psi \right), \; \text{and} \; \frac{1}{4} \Psi
    \end{equation*}
    with $0 \leq \Psi \leq 1$. The parameter $\Psi$ is to be estimated on the basis of the observed information $\bm{y}$. The probability of the observed data $\bm{y}$ is given by
    \begin{equation*}
        g (\bm{y} ; \Psi) = \frac{n!}{y_1 ! y_2 ! y_3 ! y_4 !} \left( \frac{1}{2} + \frac{1}{4} \Psi \right)^{y_1} \left( \frac{1}{4} - \frac{1}{4} \Psi \right)^{y_2} \left( \frac{1}{4} - \frac{1}{4} \Psi \right)^{y_3} \left( \frac{1}{4} \Psi \right)^{y_4}.
    \end{equation*}
    Suppose now that the first of the original four multinomial cells, which has an associated probability of $\frac{1}{2} + \frac{1}{4} \Psi$, could be split into two subcells have probabilities $\frac{1}{2}$ and $\frac{1}{4}$, respectively, and let $y_{11}$ and $y_{12}$ be the corresponding split of $y_1$, where
    \begin{equation*}
        y_1 = y_{11} + y_{12}.
    \end{equation*}
    Thus, the observed vector of frequencies $y$ is viewed as being incomplete and the complete-data vector is taken to be
    \begin{equation*}
        \bm{x} = (y_{11} , y_{12} , y_{2} , y_{3} , y_{4})^{\intercal}.
    \end{equation*}
    The cell frequencies in $\bm{x}$ are assumed to arise from a multinomial distribution having five cells with probabilities
    \begin{equation*}
        \frac{1}{2} , \frac{1}{4} \Psi , \frac{1}{4} \left( 1 - \Psi \right) , \frac{1}{4} \left( 1 - \Psi \right), \; \text{and} \; \frac{1}{4} \Psi .
    \end{equation*}
    In this framework, $y_{11}$ and $y_{12}$ are regraded as the unobservable or missing data since we only get their sum $y_1$. The complete-data log likelihood is then
    \begin{equation*}
        g_c (\bm{y} ; \Psi) = C(\bm{x}) \left( \frac{1}{2} \right)^{y_{11}}  \left( \frac{1}{4} \Psi \right)^{y_{12}} \left( \frac{1}{4} - \frac{1}{4} \Psi \right)^{y_2} \left( \frac{1}{4} - \frac{1}{4} \Psi \right)^{y_3} \left( \frac{1}{4} \Psi \right)^{y_4}
    \end{equation*}
    Thus, the complete-data log likelihood is, therefore
    \begin{equation*}
        \ln L_c (\Psi) = (y_{12} + y_{4}) \ln \Psi + (y_{2} + y_{3}) \ln (1 - \Psi) + c.
    \end{equation*}
    for some constant $c$ not involving $\Psi$. Let $\Psi^{(0)}$ be the value specified initially for $\Psi$. Then on the first iteration of the EM algorithm, the $E-$step requires the computation of the conditional expectation of $L_c (\Psi)$ given $\bm{y}$, using $\Psi^{(0)}$, which can be written as
    \begin{equation*}
        Q \left( \Psi ; \Psi^{(0)} \right) = \EE_{\Psi^{(0)}} \left[ \ln L_{c} (\Psi) \mid \bm{y} \right].
    \end{equation*}
    As $\ln L_c (\Psi)$ is a linear function of the unobservable data $y_{11}$ and $y_{12}$ for this problem, the $E-$step is effected simply by replacing $y_{11}$ and $y_{12}$ by their current conditional expectations given the observed data $\bm{y}$. Considering the random variable $Y_{11}$ corresponding to $y_{11}$, it is easy to verify that conditional on $\bm{y}$, effectively $y_1$, $Y_{11}$ has a binomial distribution with sample size $y_1$ and probability parameter
    \begin{equation*}
        \frac{1}{2} / \left( \frac{1}{2} + \frac{1}{4} \Psi^{(0)} \right),
    \end{equation*}
    where $\Psi^{(0)}$ is used in place of the unknown parameter $\Psi$. Thus the initial conditional expectation of $Y_{11}$ given $y_{1}$ is
    \begin{equation*}
        \EE_{\Psi^{(0)}} \left[ Y_{11} \mid y_1 \right] = y_{11}^{(0)},
    \end{equation*}
    where
    \begin{align*}
        y_{11}^{(0)} & = \frac{1}{2} y_{1} / \left( \frac{1}{2} + \frac{1}{4} \Psi^{(0)} \right).
    \end{align*}
    We also have
    \begin{align*}
        y_{12}^{(0)} & = \frac{1}{4} y_1 - y_{11}^{(0)}                                                      \\
                     & = \frac{1}{4} y_{1} \Psi^{(0)} / \left( \frac{1}{2} + \frac{1}{4} \Psi^{(0)} \right).
    \end{align*}
    The M-step is undertaken on the first iteration by choosing $\Psi^{(1)}$ to be the value of Q that maximizes $Q \left( \Psi ; \Psi^{(0)} \right)$ with respect to $\Psi$. Since this $Q-$function is given simply by replacing the unobservable frequencies $y_{11}$ and $y_{12}$ with their current conditional expectations $y_{11}^{(0)}$ and $y_{12}^{(0)}$ in the complete-data log likelihood, $\Psi^{(1)}$ is obtained taking the derivative of $\ln L_c (\Psi)$ to find its maximising value, that is,
    \begin{align*}
        \frac{\partial}{\partial \Psi} \ln L_c (\Psi) = 0  \
         & = \left( y_{12}^{(0)} + y_4 \right) \frac{1}{\Psi} - \left( y_2 + y_3 \right) \frac{1}{1-\Psi} \\
        \Psi \left( y_2 + y_3 \right)
         & = (1-\Psi) \left( y_{12}^{(0)} + y_4 \right)                                                   \\
        \Psi \left( y_{12}^{(0)} + y_2 + y_3 + y_4 \right)
         & = \left( y_{12}^{(0)} + y_4 \right)                                                            \\
        \Psi
         & = \frac{y_{12}^{(0)} + y_4}{y_{12}^{(0)} + y_2 + y_3 + y_4}                                    \\
         & = \frac{y_{12}^{(0)} + y_4}{n - y_{11}^{(0)}}.
    \end{align*}
    It follows on so alternating the $E-$ and $M-$steps on the $(k+1)^{th}$ iteration of the EM algorithm that
    \begin{equation*}
        \Psi^{(k+1)} = \frac{y_{12}^{(k)} + y_4}{n - y_{11}^{(k)}}
    \end{equation*}
    where
    \begin{align*}
        y_{11}^{(k)} & = \frac{1}{2} y_1 / \left( \frac{1}{2} + \frac{1}{4} \Psi^{(k)} \right) \\
        y_{12}^{(k)} & = y_1 - y_{11}^{(k)}.
    \end{align*}
\end{exam}

\begin{exam}[Bin Grouped Data (Normal)] \label{exam: em_binned_normal}
    Example taken from class notes. We consider now the application of the EM algorithm to continuous data that are grouped into
    intervals. More specifically, let $W$ be a random variable with pdf $f (w; \theta)$ specified up to a vector $\theta$ of unknown parameters. Suppose that the sample space $\calW$ of $W$ is partitioned into $v$ mutually exclusive intervals $W_j , \; (j = 1, \ldots , v)$. Independent observations are made on $W$ , but only the
    number $n_j$ falling in $W_j , \; (j = 1, \ldots , r)$ is recorded where $r \leq v$. That is, individual observations are not recorded but only the class intervals $W_j$ in which they fall are recorded; further, even such observations are made only if the $W$ value falls in one of the intervals $W_j, (j = 1, \ldots , r)$. We can solve this problem within the EM framework by introducing the vectors
    \begin{equation*}
        \boldsymbol{u}=\left(n_{r+1}, \ldots, n_{v}\right)^{T} , \quad \boldsymbol{w}_{j}=\left(w_{j 1}, \ldots, w_{j, n_{j}}\right)^{T} \quad(j=1, \ldots, v)
    \end{equation*}
    as the missing data. The vector $\bm{u}$ contains the unobservable frequencies in the case of truncation $(r < v)$, while $\bm{w}_j$ contains the $n_j$ unobservable individual observations in the jth interval $W_j , \; (j = 1, \ldots , v)$. The complete-data vector $\bm{x}$ corresponding to the missing data is
    \begin{equation*}
        \boldsymbol{x}=\left(\boldsymbol{y}^{T}, \boldsymbol{u}^{T}, \boldsymbol{w}_{1}^{T}, \ldots, \boldsymbol{w}_{v}^{T}\right)^{T}.
    \end{equation*}
    The complete-data log likelihood function for $\bm{\theta} , \; \ln L_c (\bm{\theta})$ is
    \begin{equation*}
        \log L_{c} (\bm{\theta}) = \sum_{j=1}^{v} \sum_{l=1}^{n_j} \log f \left( w_{jl} ; \bm{\theta} \right).
    \end{equation*}
    To perform the $E-$step, $Q$ can be computed to be
    \begin{align*}
        Q \left( \bm{\theta}; \bm{\theta}^{(k)} \right) \
         & = \mathbb{E}_{\bm{\theta}^{(k)}} \left[ \log L_{c} (\bm{\theta}) \mid \bm{y} \right]                                                                                                           \\
         & = \mathbb{E}_{\bm{\theta}^{(k)}} \left[ \sum_{j=1}^{v} \sum_{l=1}^{n_j} \log f \left( W_{jl} ; \bm{\theta} \right) \mid \bm{y} \right]                                                         \\
         & = \mathbb{E}_{\bm{\theta}^{(k)}} \left[ \mathbb{E}_{\bm{\theta}^{(k)}} \left[ \sum_{j=1}^{v} \sum_{l=1}^{n_j} \log f \left( W_{jl} ; \bm{\theta} \right) \mid \bm{y} , u \right]\right]        \\
         & = \sum_{j=1}^{v} \mathbb{E}_{\bm{\theta}^{(k)}} \left[ n_j \mid \bm{y} \right] \mathbb{E}_{\bm{\theta}^{(k)}} \left[  \log f \left( W ; \bm{\theta} \right) \mid W \in \mathcal{W}_{j} \right] \\
         & = \sum_{j=1}^{v} n_{j}^{(k)} Q_j (\bm{\theta} , \bm{\theta}^{(k)})
    \end{align*}
    where
    \[
        Q_j (\bm{\theta} , \bm{\theta}^{(k)}) = \mathbb{E}_{\bm{\theta}^{(k)}} \left[ \log f (W;\bm{\theta}) \mid W \in \mathcal{W}_{j} \right]
    \]
    and
    \begin{align*}
        n_{j}^{(k)} \
         & = n_j, \quad 1 \leq j \leq r                                                                                                     \\
         & = \mathbb{E}_{\bm{\theta}^{(k)}} \left[ n_j \mid \bm{y} \right] = \frac{n P_{v}(\bm{\theta}^{(k)})}{P(\bm{\theta}^{(k)})}, j=v .
    \end{align*}
    To perform the $M-$step,
    \begin{equation*}
        \partial Q\left(\boldsymbol{\theta} ; \boldsymbol{\theta}^{(k)}\right) / \partial \boldsymbol{\theta}=\sum_{j=1}^{v} n_{j}^{(k)} \partial Q_{j}\left(\boldsymbol{\theta} ; \boldsymbol{\theta}^{(k)}\right) / \partial \boldsymbol{\theta}
    \end{equation*}
    where
    \begin{equation*}
        \partial Q_{j}\left(\boldsymbol{\theta} ; \boldsymbol{\theta}^{(k)}\right) / \partial \boldsymbol{\theta}=E_{\boldsymbol{\theta}^{(k)}}\left\{\partial \log f(W ; \boldsymbol{\theta}) / \partial \boldsymbol{\theta} \mid W \in \mathcal{W}_{j}\right\}
    \end{equation*}
    on interchanging the operations of differentiation and expectation. In this case
    \begin{align*}
        Q_j (\bm{\theta} , \bm{\theta}^{(k)}) \
         & = \mathbb{E}_{\bm{\theta}^{(k)}} \left[ \log \left( \frac{1}{\sigma \sqrt{2 \pi}} \exp \left( -\frac{1}{2} \left( \frac{W - \mu}{\sigma} \right)^2 \right) \right) \mid W \in \mathcal{W}_{j} \right]              \\
         & = \mathbb{E}_{\bm{\theta}^{(k)}} \left[ - \frac{1}{2} \log \left( 2 \pi \right) - \frac{1}{2} \log \left( \sigma^2 \right) -\frac{1}{2} \left( \frac{W - \mu}{\sigma} \right)^2 \mid W \in \mathcal{W}_{j} \right] \\
         & = - \frac{1}{2} \left( \log \left( 2 \pi \right) + \log \left( \sigma^2 \right) \right) - \frac{\sigma^{-2}}{2} \mathbb{E}_{\bm{\theta}^{(k)}} \left[ \left( W - \mu \right)^2 \mid W \in \mathcal{W}_{j} \right]  \\
    \end{align*}
    so that
    \begin{align*}
        Q \left( \bm{\theta}; \bm{\theta}^{(k)} \right) \
         & = \sum_{j=1}^{v} n_{j}^{(k)} Q_j (\bm{\theta} , \bm{\theta}^{(k)})                                                                                                                                                                                                       \\
         & = \sum_{j=1}^{v} n_{j}^{(k)} \left[ - \frac{1}{2} \left( \log \left( 2 \pi \right) + \log \left( \sigma^2 \right) \right) - \frac{\sigma^{-2}}{2} \mathbb{E}_{\bm{\theta}^{(k)}} \left[ \left( W - \mu \right)^2 \mid W \in \mathcal{W}_{j} \right] \right]              \\
         & = - \frac{1}{2} \sum_{j=1}^{v} n_{j}^{(k)} \left( \log \left( 2 \pi \right) + \log \left( \sigma^2 \right) \right) - \frac{\sigma^{-2}}{2} \sum_{j=1}^{v} n_{j}^{(k)} \mathbb{E}_{\bm{\theta}^{(k)}} \left[ \left( W - \mu \right)^2 \mid W \in \mathcal{W}_{j} \right].
    \end{align*}
    To complete the $M-$step, from the above we have
    \begin{align*}
        \partial Q_{j}\left(\boldsymbol{\theta} ; \boldsymbol{\theta}^{(k)}\right) / \partial \mu \
         & = \EE_{\boldsymbol{\theta}^{(k)}}\left\{\partial \log f(W ; \boldsymbol{\theta}) / \partial \mu \mid W \in \mathcal{W}_{j}\right\}                 \\
         & = \frac{\sigma^{-2}}{2} \sum_{j=1}^{v} n_{j}^{(k)} \mathbb{E}_{\bm{\theta}^{(k)}} \left[ \left( W - \mu \right) \mid W \in \mathcal{W}_{j} \right]
    \end{align*}
    so that
    \begin{align*}
        \frac{\sigma^{-2}}{2} \sum_{j=1}^{v} n_{j}^{(k)} \mathbb{E}_{\bm{\theta}^{(k)}} \left[ \left( W - \mu^{(k+1)} \right) \mid W \in \mathcal{W}_{j} \right] & = 0                                                                                                                                                     \\
        \sum_{j=1}^{v} n_{j}^{(k)} \mathbb{E}_{\bm{\theta}^{(k)}} \left[ \left( W - \mu^{(k+1)} \right) \mid W \in \mathcal{W}_{j} \right]                       & = 0                                                                                                                                                     \\
        \sum_{j=1}^{v} n_{j}^{(k)} \mathbb{E}_{\bm{\theta}^{(k)}} \left[ W \mid W \in \mathcal{W}_{j} \right] - \mu^{(k+1)} \sum_{j=1}^{v} n_{j}^{(k)}           & = 0                                                                                                                                                     \\
        \mu^{(k+1)}                                                                                                                                              & = \sum_{j=1}^{v} n_{j}^{(k)} \mathbb{E}_{\bm{\theta}^{(k)}} \left[ W \mid W \in \mathcal{W}_{j} \right] \left( \sum_{j=1}^{v} n_{j}^{(k)} \right)^{-1}.
    \end{align*}
    Do something similar for ${\sigma^{2}}^{(k+1)}$ provides an iterate of
    \begin{equation*}
        \sigma^{(k+1)^{2}}=\sum_{j=1}^{v} n_{j}^{(k)}\left[\EE_{\theta^{(k)}}\left\{\left(W-\mu^{(k+1)}\right)^{2} \mid W \in \mathcal{W}_{j}\right\}\right] \left( \sum_{j=1}^{v} n_{j}^{(k)} \right)^{-1}.
    \end{equation*}
\end{exam}